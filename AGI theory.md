# **_AGI Theory_**  
### Spearheaded by: **_Grathium Research_**
---

**To develop an AGI the following modules need to be developed**
> Memory w/r (✓)  
> Memory Stacking (✓)  
> Computational Threading (✓)  
> Deep Neural Networks (✓)  
> IO Scrapping (partial)  
> Unique Idea Generation (Undeveloped)  
> Recursive Deep Neural Networks (undeveloped)  
> **AGI**  
  
### _What does this mean_?
After breaking down to the base of what AGI would look like. We are currently missing _Three_ of the crucial elements of AGI.  
  
IO Scrapping is the idea of scrapping and letting the AGI gather what it thinks should be the input and output for the experiment. In previously developed AI, the AI has been given input and output models to work off of. Giving a determined IO inhibits AGI from developing due to it not being able to create and use it's own form of IO.  
  
Unique idea generation is the hardest to develop. The theory of developing a unique idea is almost impossible for the human mind to comprehend. An algorithmic equation may combine may other accomplishments, but unique idea generation will be the foundation of future AGI.
  
Recursive Deep Neural Networks is the name given to a neural network that has neural networks that can be called upon by higher order Deep Neural Networks. Recursive Deep Neural Networks differ because they can be infinitely expanded upon when learning.